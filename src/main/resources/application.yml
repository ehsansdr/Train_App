spring:
  profiles:
    active: postgres

  kafka:
    bootstrap-servers: 192.168.110.243:19091 # Kafka broker address(es)
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer # Serializer for the key of the Kafka message (String in this case)
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer # Serializer for the value of the Kafka message (using Spring's JsonSerializer for JSON)
      properties:
        spring:
          kafka:
            json:
              add:
                type:
                  headers: true # This property tells the JsonSerializer to add type information to the message headers
    consumer:
      bootstrap-servers: 192.168.110.243:19091 # Replace with your Kafka brokers
      group-id: ${my.kafka.consumer-group-id}         # Replace with your desired group ID
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer # Deserializer for the key of the Kafka message (String in this case)
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer # Deserializer for the value of the Kafka message (using Spring's JsonDeserializer for JSON)
      auto-offset-reset: latest             # Or earliest
      enable-auto-commit: true             # Default is true
      auto-commit-interval: 5000           # in milliseconds (default: 5000)
      max-poll-records: 500                # Maximum number of records to fetch in a single poll
      fetch-min-bytes: 1024                # Minimum amount of data the server should return for a fetch request
      fetch-max-wait: 1000                # Maximum amount of time the server will block before answering the fetch request if there isn't sufficient data
      properties:
        spring:
          json:
            value:
              default:
                type: com.example.trainproject.base.Model.KafkaProduceMessage # Explicitly set the default type for deserialization if no type header is found
            trusted:
              packages: com.example.trainproject.base.Model # Add the package of your KafkaProduceMessage class to allow deserialization
              # Add other trusted package names here, separated by commas, if you have other models to consume

  liquibase:
    enabled: true
    change-log: /db/changelog/db.yml
#  cloud:
#    vault:
#      uri: http://127.0.0.1:8200
#      token: hvs.TH8eeOix4dlffTyctwiypM8K
#      kv:
#        enabled: true
#        backend: secret
#        default-context: foo  # Correct path to the secret
#spring.config.import: vault:// # it is essential to have this , if not the application will not find the context in the secret

server:
  port: 8080

logging:
  level:
    liquibase: DEBUG
    root: INFO
    org.hibernate.SQL: DEBUG
    org.hibernate.type.descriptor.sql: TRACE
    org.springdoc: DEBUG

springdoc:
  swagger-ui:
    enabled: true
    layout: StandaloneLayout
    title: My Custom Swagger UI
    deep-linking: true
    url: /v3/api-docs
    css-url: /swagger-ui.css

Train:
  enableJob: false

my:
  kafka:
    topic: "kafka_training_Es"
    consumer-group-id: "my-consumer-group-id"
